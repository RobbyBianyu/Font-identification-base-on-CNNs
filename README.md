# Font-identification-base-on-CNNs
In this section, we explored the task of font recognition from images of pangram sentences. Our goal was to build a convolutional neural network (CNN) that classifies small image patches extracted from rendered sentences into their corresponding font styles.
## Data Preparation
Our pipeline began by uncompressing a ZIP archive of rendered pangram images, then identifying and removing duplicates using SHA-256 hashing on grayscale pixel data. This ensured a clean and diverse collection of unique font-style samples for downstream processing.
In order to get enough samples to train the model, each retained image was split into two horizontal halves, and white margins were trimmed by analyzing pixel intensity boundaries. From each half, up to thirty vertical slices of fixed width were randomly sampled, subject to a minimum five-percent threshold of non-white pixels. These slices were resized to 16×16 pixels and flattened into one-dimensional feature vectors. Since there are too many classes of font, one thousand font styles were randomly subsampled to enable efficient training.
## Model Setup
The dataset was partitioned into training, validation, and test sets in an 80%/10%/10% randomly. Each patch was converted to a PyTorch tensor and normalized to the range [–1, +1]. The core model, “SimpleCNN”, consisted of three convolutional blocks which includes expanding filters from 1→64→128→256 with batch normalization, ReLU activations, max pooling, and dropout for regularization. In addition, a two-layer fully connected head produced logits over the font-style classes. For this part, we used a rented A100 GPU because it's hardware-accelerated BFloat16 significantly improved training efficiency. Thus, we primarily used BFloat16 to accelerate training in this part.
## Hyperparameter Search and Test Set Evaluation
We conducted a grid search over learning rates [0.01, 0.001, 0.0001] and batch sizes [16, 32, 64, 128], and training each configuration for up to 50 epochs with early stopping (patience of five epochs) based on validation loss. Optimization employed SGD with momentum (0.9) and weight decay (1×10⁻⁴). The optimal model was evaluated using test dataset and yielded a test accuracy.
Table 3.1. below summarizes the validation accuracies across different hyperparameter configurations. The model with a learning rate of 0.01 and batch size of 64 achieved the highest validation accuracy, aligning with the selected optimal model for testing.
Further evaluation using the ROC curves (also shows below) confirms the model’s excellent classification ability. Most configurations achieved near-perfect area under the curve (AUC) values, particularly for larger learning rates and smaller batch sizes. Performance degradation was noticeable only under extremely small learning rates (0.0001) combined with larger batch sizes (128).
![image](https://github.com/user-attachments/assets/32e2d2bb-e572-4eb5-b863-981b2f507ab1)
![image](https://github.com/user-attachments/assets/3be9cd07-0fc3-4bd6-af67-0fa1be4d4837)
